# -*- coding: utf-8 -*-
"""ML_INTERNSHIP.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/11wxozpNobgq_1VPdPHFvOOEbSeLk-vhl
"""

!pip install diffusers==0.31.0 transformers accelerate safetensors torch --quiet
!pip install streamlit pillow tqdm realesrgan gdown --quiet
!pip install xformers --quiet --upgrade
#Install Dependencies (GPU/CPU Auto-Detect)

#Load Stable Diffusion Model
import torch
from diffusers import StableDiffusionPipeline

device = "cuda" if torch.cuda.is_available() else "cpu"
print("Running on:", device)

model_id = "runwayml/stable-diffusion-v1-5"

pipe = StableDiffusionPipeline.from_pretrained(
    model_id,
    torch_dtype=torch.float16 if device=="cuda" else torch.float32,
    safety_checker=None  # custom safety implemented later
)

if device == "cuda":
    pipe.enable_xformers_memory_efficient_attention()
    pipe.to("cuda")
else:
    pipe.to("cpu")

#Create Safety Filter (NSFW Check)
from transformers import CLIPProcessor, CLIPModel
import torch.nn.functional as F

clip_model = CLIPModel.from_pretrained("openai/clip-vit-base-patch32").to(device)
clip_processor = CLIPProcessor.from_pretrained("openai/clip-vit-base-patch32")

def nsfw_filter(image):
    inputs = clip_processor(images=image, return_tensors="pt").to(device)
    with torch.no_grad():
        logits = clip_model.get_image_features(**inputs)
    score = float(logits.sum())
    return score < 0  # threshold heuristic

#Generation Function with Metadata Saving
import time, os, json
from PIL import Image

def generate_and_save(prompt, negative_prompt, num_steps, guidance, width, height, seed=None):
    generator = torch.Generator(device=device)
    if seed is not None:
        generator.manual_seed(seed)

    output = pipe(prompt=prompt,
                  negative_prompt=negative_prompt,
                  num_inference_steps=num_steps,
                  guidance_scale=guidance,
                  width=width,
                  height=height,
                  generator=generator).images[0]

    # Safety Check
    if nsfw_filter(output):
        raise Exception("NSFW Content detected. Try different prompt.")

    ts = int(time.time())
    os.makedirs("generated/images", exist_ok=True)
    os.makedirs("generated/metadata", exist_ok=True)

    filename = f"generated/images/{ts}.png"
    output.save(filename)

    metadata = {
        "prompt": prompt,
        "negative_prompt": negative_prompt,
        "steps": num_steps,
        "guidance": guidance,
        "width": width,
        "height": height,
        "seed": seed,
        "timestamp": ts
    }
    with open(f"generated/metadata/{ts}.json", "w") as f:
        json.dump(metadata, f, indent=4)

    return filename

#Optional Upscaler (Real-ESRGAN)
def upscale_image(image_path):
    from subprocess import run
    upscaled_path = image_path.replace(".png", "_up.png")
    run(f"realesrgan-ncnn-vulkan -i {image_path} -o {upscaled_path}", shell=True)
    return upscaled_path

# Commented out IPython magic to ensure Python compatibility.
# #Build Streamlit Web UI
# %%writefile app.py
# import streamlit as st
# from PIL import Image
# import os
# from main import generate_and_save, upscale_image
# 
# st.title(" AI-Powered Text to Image Generator (Stable Diffusion) ")
# 
# prompt = st.text_input("Enter Prompt:", "A futuristic city at sunset, ultra realistic")
# negative_prompt = st.text_input("Negative Prompt:", "blurry, lowres, watermark")
# steps = st.slider("Inference Steps:", 10, 100, 40)
# guidance = st.slider("Guidance Scale:", 1.0, 15.0, 7.5)
# width = st.selectbox("Width:", [512, 768, 1024])
# height = st.selectbox("Height:", [512, 768, 1024])
# 
# upscale = st.checkbox("Upscale Image (Optional)")
# 
# if st.button("Generate"):
#     with st.spinner("Generating..."):
#         try:
#             img_path = generate_and_save(prompt, negative_prompt, steps, guidance, width, height)
#             st.image(img_path, caption="Generated Image")
# 
#             if upscale:
#                 up_path = upscale_image(img_path)
#                 st.image(up_path, caption="Upscaled Image")
# 
#             st.success("Done!")
# 
#         except Exception as e:
#             st.error(str(e))
#

# Commented out IPython magic to ensure Python compatibility.
# %%writefile main.py
# from app import *
#

!streamlit run app.py --server.port 6006 & npx localtunnel --port 6006

from pyngrok import ngrok
ngrok.set_auth_token("360QQO3tjj9eASFk81wxtdrg7Uo_5K2VuEUMpALinnaYw7t74")
NGROK_AUTH_TOKEN = "360QQO3tjj9eASFk81wxtdrg7Uo_5K2VuEUMpALinnaYw7t74"

import subprocess, threading

# This cell assumes app.py is already defined as a Streamlit application.
# If app.py is not yet created or correctly written, the Streamlit app will not start.
# The app.py content was last updated in cell `tKiChpnZ_TnP`.

def run():
    subprocess.run(["streamlit", "run", "app.py", "--server.port=8501", "--server.headless=True"])

# Run Streamlit in a separate thread to not block the Colab environment
threading.Thread(target=run).start()

!pip install pyngrok streamlit -q
from pyngrok import ngrok
import subprocess, threading

NGROK_AUTH_TOKEN = "360QQO3tjj9eASFk81wxtdrg7Uo_5K2VuEUMpALinnaYw7t74"  # Must start with 2

ngrok.set_auth_token(NGROK_AUTH_TOKEN)
ngrok.kill()

public_url = ngrok.connect(8501)
print("Public URL:", public_url)

with open("app.py", "w") as f:
    f.write("import streamlit as st\nst.title('Streamlit Works!')")

def run():
    subprocess.run(["streamlit", "run", "app.py", "--server.port=8501"])

threading.Thread(target=run).start()